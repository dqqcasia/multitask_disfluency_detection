{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from swda import swda\n",
    "from swda_utils import (extract_nonspeech,\n",
    "                        filter_nonspeech,\n",
    "                        extract_nonsentence,\n",
    "                        filter_nonsentence,\n",
    "                        extract_restarts_with_repair_and_nonsentence,\n",
    "                        filter_restarts_with_repair_and_nonsentence,\n",
    "                        extract_restarts_with_repair,\n",
    "                        filter_restarts_with_repair,\n",
    "                        extract_restarts_without_repair,\n",
    "                        filter_restarts_without_repair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = swda.CorpusReader('swda/swda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('{F...}', ('{F uh, }', 'uh, '))]\n",
      "('Actually, uh, ', 'Actually, ')\n"
     ]
    }
   ],
   "source": [
    "print extract_nonsentence('Actually, {F uh, }')\n",
    "print filter_nonsentence('Actually, {F uh, }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[RM + {} RR]', ('[ I, + {F uh, } I]', ' I,', '{F uh, }', 'F', 'uh, ', 'I'))]\n",
      "('Actually, I, uh, I guess I am [I, + I], {F uh, }', 'Actually, I guess I am [I, + I], {F uh, }')\n"
     ]
    }
   ],
   "source": [
    "print extract_restarts_with_repair_and_nonsentence('Actually, [ I, + {F uh, } I] guess I am [I, + I], {F uh, }')\n",
    "print filter_restarts_with_repair_and_nonsentence('Actually, [ I, + {F uh, } I] guess I am [I, + I], {F uh, }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[RM + RR]', ('[I, + I]', 'I,', 'I'))]\n",
      "('Actually, [ I, + {F uh, } I] guess I am I, I, {F uh, }', 'Actually, [ I, + {F uh, } I] guess I am I, {F uh, }')\n"
     ]
    }
   ],
   "source": [
    "print extract_restarts_with_repair('Actually, [ I, + {F uh, } I] guess I am [I, + I], {F uh, }')\n",
    "print filter_restarts_with_repair('Actually, [ I, + {F uh, } I] guess I am [I, + I], {F uh, }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[RM +]', ('[ I + ]', ' I'))]\n",
      "('Actually, I guess I am [I, + I], {F uh, }', 'Actually, guess I am [I, + I], {F uh, }')\n"
     ]
    }
   ],
   "source": [
    "print extract_restarts_without_repair('Actually, [ I + ] guess I am [I, + I], {F uh, }')\n",
    "print filter_restarts_without_repair('Actually, [ I + ] guess I am [I, + I], {F uh, }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    (extract_restarts_with_repair_and_nonsentence, filter_restarts_with_repair_and_nonsentence),\n",
    "    (extract_restarts_with_repair, filter_restarts_with_repair),\n",
    "    (extract_restarts_without_repair, filter_restarts_without_repair),\n",
    "    (extract_nonsentence, filter_nonsentence),\n",
    "]\n",
    "disfluency_stats = defaultdict(lambda: 0)\n",
    "parallel_corpus = []\n",
    "for utt in reader.iter_utterances(display_progress=False):\n",
    "    local_disfluencies = defaultdict(lambda: 0)\n",
    "    utt_filtered = filter_nonspeech(utt.text)\n",
    "    utt_original, utt_clean = utt_filtered, utt_filtered\n",
    "    for extract_step, filter_step in pipeline:\n",
    "        disfluencies = extract_step(utt_original)\n",
    "        if not len(disfluencies):\n",
    "            continue\n",
    "        utt_original, utt_clean = filter_step(utt_original)[0], filter_step(utt_clean)[1]\n",
    "        if not len(re.findall('\\w+', utt_clean)):\n",
    "            break\n",
    "        for disfluency_type, disfluency in disfluencies:\n",
    "            local_disfluencies[disfluency_type] += 1\n",
    "    if local_disfluencies and len(re.findall('\\w+', utt_clean)):\n",
    "        for disfluency_type, count in local_disfluencies.iteritems():\n",
    "            disfluency_stats[disfluency_type] += count\n",
    "        parallel_corpus.append((utt_original, utt_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 30% of utternaces in the final corpus will be fluent\n",
    "fluent_corpus_size = int(0.3 * len(parallel_corpus) / 0.7)\n",
    "parallel_corpus_fluent = []\n",
    "for utt in reader.iter_utterances(display_progress=False):\n",
    "    utt_filtered = filter_nonspeech(utt.text)\n",
    "    utt_original, utt_clean = utt_filtered, utt_filtered\n",
    "    fluent = True\n",
    "    for extract_step, filter_step in pipeline:\n",
    "        disfluencies = extract_step(utt_original)\n",
    "        if not len(disfluencies):\n",
    "            fluent = False\n",
    "            break\n",
    "    if fluent and len(re.findall('\\w+', utt_clean)):\n",
    "        parallel_corpus_fluent.append((utt_clean, utt_clean))\n",
    "    if fluent_corpus_size == len(parallel_corpus_fluent):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus stats\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus = [(nltk.word_tokenize(utt_from.translate(None, string.punctuation)), nltk.word_tokenize(utt_to.translate(None, string.punctuation)))\n",
    "                for utt_from, utt_to in parallel_corpus + parallel_corpus_fluent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of utterances with disfluencies: 95900\n",
      "Total number of utterances without disfluencies: 99\n",
      "Mean utterance length (utterance_from): 10.828\n",
      "Mean utterance length (utterance_to): 8.696\n",
      "Dislfuency stats by type:\n",
      "[RM + {} RR]:\t6467\n",
      "{E...}:\t3257\n",
      "{A...}:\t294\n",
      "{C...}:\t48083\n",
      "{D...}:\t28696\n",
      "{F...}:\t39464\n",
      "[RM + RR]:\t30532\n",
      "[RM +]:\t2964\n"
     ]
    }
   ],
   "source": [
    "print 'Total number of utterances with disfluencies: {}'.format(len(parallel_corpus))\n",
    "print 'Total number of utterances without disfluencies: {}'.format(len(parallel_corpus_fluent))\n",
    "print 'Mean utterance length (utterance_from): {:.3f}'.format(np.mean([len(utt_from) for utt_from, utt_to in final_corpus]))\n",
    "print 'Mean utterance length (utterance_to): {:.3f}'.format(np.mean([len(utt_to) for utt_from, utt_to in final_corpus]))\n",
    "print 'Dislfuency stats by type:'\n",
    "for key, value in disfluency_stats.iteritems():\n",
    "    print '{}:\\t{}'.format(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts = []\n",
    "for utt in reader.iter_utterances(display_progress=False):\n",
    "    if extract_restarts_with_repair_and_nonsentence(utt.text):\n",
    "        utts.append(utt)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{C But, } {F uh, } I bought it for target practicing [ and, + {F uh, } and ] also because I wanted a weapon.  /'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utts[0].text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
